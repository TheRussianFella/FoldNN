{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from Protein_Dataset import Protein_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-28fb48be3662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProtein_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/max/Documents/Bio/ProtSys/Protein_Dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_path, test_path, maxlen, stride)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mseq\u001b[0m \u001b[0melements\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mtaken\u001b[0m \u001b[0minto\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprevious\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/max/Documents/Bio/ProtSys/Protein_Dataset.py\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self, dir_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mdssp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".dssp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mfasta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".fasta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "ds = Protein_Dataset(\"train/\", \"test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_prediction(part_predictions, original_length, stride):\n",
    "    max_len = part_predictions.shape[1]\n",
    "    \n",
    "    final_prediction = []\n",
    "    \n",
    "    if len(part_predictions) != 1:\n",
    "        final_prediction = np.concatenate(part_predictions[:-1], axis=0)\n",
    "\n",
    "    el_left = original_length % max_len\n",
    "    start_element = stride\n",
    "    \n",
    "    if el_left + stride > max_len:\n",
    "        start_element = max_len - el_left\n",
    "\n",
    "    final_part_prediction = part_predictions[-1][start_element:start_element + el_left]\n",
    "\n",
    "    final_prediction = np.concatenate([final_prediction, final_part_prediction], axis=0)\n",
    "    \n",
    "    return final_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(x, session, method, maxlen=400, stride=100):\n",
    "    prepared = []; idx = []; original_len = []; model_len = []\n",
    "    \n",
    "    curr_idx = 0\n",
    "    \n",
    "    for seq in x:\n",
    "        original_len.append(len(seq))\n",
    "        \n",
    "        splited = split_seq(seq, maxlen, stride)\n",
    "        \n",
    "        model_len.extend(list(map(len, splited)))\n",
    "        splited = sequence.pad_sequences(splited, maxlen, value=-1, padding=\"post\")\n",
    "        \n",
    "        prepared.extend(splited)\n",
    "        idx.append([curr_idx, curr_idx+len(splited)])\n",
    "        curr_idx += len(splited)\n",
    "    \n",
    "    predictions = session.run(method, feed_dict={\n",
    "        x_raw: np.array(prepared),\n",
    "        seq_len: np.array(model_len)\n",
    "    })\n",
    "    \n",
    "    final_predictions = []\n",
    "    for i, seq_idx in enumerate(idx):\n",
    "        seq_predictions = predictions[slice(seq_idx[0], seq_idx[1])]\n",
    "\n",
    "        final_predictions.append(build_prediction(seq_predictions, original_len[i], stride))\n",
    "        \n",
    "    return np.array(list(map(lambda x:make_pred(x, len(x), class_table), final_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(x, y, session, method, maxlen=400, stride=100):\n",
    "    y_pred = predict(x, session, method)\n",
    "    y_pred = list(map(lambda x: list(map(lambda z: class_table.index(z) + 1, x)), y_pred))\n",
    "    return np.mean([accuracy_score(y[i], y_pred[i]) for i in range(len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc_score(y_true, y_pred):\n",
    "    y_pred = list(map(lambda x: list(map(lambda z: class_table.index(z) + 1, x)), y_pred))\n",
    "    return np.mean([accuracy_score(y_true[i], y_pred[i]) for i in range(len(y_true))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_class = len(acid_table)\n",
    "output_class = len(class_table) + 1 # plus one for padding\n",
    "\n",
    "learning_rate = 0.01\n",
    "seq_max_len = 400\n",
    "n_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_raw = tf.placeholder(tf.int32, [None, seq_max_len], name=\"x_raw\")\n",
    "y_raw = tf.placeholder(tf.int32, [None, seq_max_len], name=\"y_raw\")\n",
    "seq_len = tf.placeholder(tf.int32, [None], name=\"seq_len\")\n",
    "\n",
    "x = tf.one_hot(x_raw, input_class, dtype=tf.float32, name=\"one_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(reuse=tf.get_variable_scope().reuse):\n",
    "    cell = tf.contrib.rnn.LSTMCell(n_units, reuse=reuse)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fw_cell = lstm_cell(); bw_cell = lstm_cell()\n",
    "\n",
    "output, state = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, x, seq_len, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con_out = tf.concat(output, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = tf.reshape(con_out, [-1, 2 * n_units * seq_max_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1_num_units = int(np.mean([2 * n_units * seq_max_len, output_class * seq_max_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"h1\"  : tf.Variable(tf.truncated_normal(shape=[2 * n_units * seq_max_len, h1_num_units], stddev=0.1)),\n",
    "    \"out\" : tf.Variable(tf.truncated_normal(shape=[h1_num_units, output_class * seq_max_len], stddev=0.1))\n",
    "}\n",
    "bias = {\n",
    "    \"h1\"  : tf.Variable(tf.truncated_normal(shape=[h1_num_units])),\n",
    "    \"out\" : tf.Variable(tf.truncated_normal(shape=[output_class * seq_max_len]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1 = tf.nn.relu(tf.matmul(flatten, weights[\"h1\"]) + bias[\"h1\"])\n",
    "flat_logits = tf.matmul(h1, weights[\"out\"]) + bias[\"out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.reshape(flat_logits, shape=[-1, seq_max_len, output_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_raw, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = tf.to_float(tf.not_equal(y_raw, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_loss = loss * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = tf.nn.l2_loss(weights['h1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss = tf.reduce_mean(tf.reduce_sum(norm_loss) / tf.to_float(seq_len) + l2_reg * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tf.arg_max(tf.nn.softmax(logits), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "disp_batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        avg_loss = 0.; num_iter = 0\n",
    "        for x_batch, y_batch, len_batch in dynamic_iter(train, batch_size, maxlen=seq_max_len):\n",
    "            \n",
    "            _, c = sess.run([train_step, mean_loss], feed_dict={\n",
    "                x_raw: x_batch,\n",
    "                y_raw: y_batch,\n",
    "                seq_len: len_batch\n",
    "            })\n",
    "            \n",
    "            num_iter += 1; avg_loss += c\n",
    "            \n",
    "            if num_iter % disp_batch == 0:\n",
    "                \n",
    "                test_pred = predict(test[:, 0], sess, pred)\n",
    "                \n",
    "                print(\"Batch {}\".format(num_iter))\n",
    "                print(\"Example: \")\n",
    "                \n",
    "                dem_idx = np.random.randint(0, len(test))\n",
    "                \n",
    "                print(\"Real:      \" + make_pred(test[dem_idx, 1], len(test[dem_idx, 1]), class_table))\n",
    "                \n",
    "                print(\"Predicted: \" + test_pred[dem_idx])\n",
    "                \n",
    "                score = acc_score(test[:, 1], test_pred)\n",
    "                \n",
    "                print(\"Accuracy: \" + str(score))\n",
    "                \n",
    "        print(\"Epoch {} done! Average loss: \".format(str(e)) + str(avg_loss / num_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82065223269391485"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_net_pred = []\n",
    "for name in test_names:\n",
    "    j_net_pred.append(\"\".join(read_file(os.path.join(\"JNet_data/\", name+\".jnet\"))[1].strip(\"\\njnetpred:\").split(\",\")))\n",
    "\n",
    "acc_score(test[:, 1], j_net_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
